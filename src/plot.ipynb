{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.set_loglevel(\"info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files\n",
    "df_file = \"results/evaluation_all.tsv\"\n",
    "out_folder = \"results\"\n",
    "\n",
    "# Set to None if you don't want to use it. Results will not be grouped/filtered by team\n",
    "names_file = None\n",
    "\n",
    "# Cumulate the last column of the cols variable, e.g. \"pr\" --> precision, so that the curves are monotonic as in CAFA\n",
    "cumulate = True\n",
    "\n",
    "# Add extreme points to the precision-recall curves (0, 1) and (1, 0)\n",
    "add_extreme_points = True\n",
    "\n",
    "# Methods with coverage below this threshold will not be plotted\n",
    "coverage_threshold = 0.3\n",
    "\n",
    "# Select a metric\n",
    "# metric, cols = ('f', ['rc', 'pr'])\n",
    "metric, cols =  ('f_w', ['rc_w', 'pr_w'])\n",
    "# metric, cols =  ('f_micro', ['rc_micro', 'pr_micro'])\n",
    "# metric, cols =  ('f_micro_w', ['rc_micro_w', 'pr_micro_w'])\n",
    "# metric, cols = ('s_w', ['ru_w', 'mi_w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map column names to full names (for axis labels)\n",
    "axis_title_dict = {'pr': 'Precision', 'rc': 'Recall', 'f': 'F-score', 'pr_w': 'Weighted Precision', 'rc_w': 'Weighted Recall', 'f_w': 'Weighted F-score', 'mi': 'Misinformation (Unweighted)', 'ru': 'Remaining Uncertainty (Unweighted)', 'mi_w': 'Misinformation', 'ru_w': 'Remaining Uncertainty', 's': 'S-score', 'pr_micro': 'Precision (Micro)', 'rc_micro': 'Recall (Micro)', 'f_micro': 'F-score (Micro)', 'pr_micro_w': 'Weighted Precision (Micro)', 'rc_micro_w': 'Weighted Recall (Micro)', 'f_micro_w': 'Weighted F-score (Micro)'}\n",
    "\n",
    "# Map ontology namespaces to full names (for plot titles)\n",
    "ontology_dict = {'biological_process': 'BPO', 'molecular_function': 'MFO', 'cellular_component': 'CCO'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(df_file, separator=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set method information (optional)\n",
    "if names_file is None:\n",
    "    df = df.with_columns([\n",
    "        pl.col('filename').alias('group'),\n",
    "        pl.col('filename').alias('label'),\n",
    "        pl.lit(False).alias('is_baseline'),\n",
    "    ])\n",
    "else:\n",
    "    methods = pl.read_csv(names_file, separator=\" \")\n",
    "    df = df.join(methods, on='filename', how='left')\n",
    "    df = df.with_columns([\n",
    "        pl.col('group').fill_null(pl.col('filename')),\n",
    "        pl.col('label').fill_null(pl.col('filename')),\n",
    "    ])\n",
    "    if 'is_baseline' not in df.columns:\n",
    "        df = df.with_columns(pl.lit(False).alias('is_baseline'))\n",
    "    else:\n",
    "        df = df.with_columns(pl.col('is_baseline').fill_null(False))\n",
    "\n",
    "df = df.drop('filename')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by coverage\n",
    "df = df.filter(pl.col('cov') >= coverage_threshold)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors based on group\n",
    "cmap = plt.get_cmap('tab20')\n",
    "# Create a mapping from unique groups to color indices\n",
    "unique_groups = df['group'].unique().to_list()\n",
    "group_to_color_idx = {g: i for i, g in enumerate(unique_groups)}\n",
    "# Map groups to colors\n",
    "color_indices = df['group'].map_elements(lambda x: group_to_color_idx[x], return_dtype=pl.Int64)\n",
    "colors = color_indices.map_elements(lambda x: cmap.colors[x % len(cmap.colors)], return_dtype=pl.Object)\n",
    "df = df.with_columns(colors.alias('colors'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best methods and thresholds\n",
    "if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "    index_best = df.group_by(['group', 'ns']).agg(pl.all().sort_by(metric).last())\n",
    "else:\n",
    "    index_best = df.group_by(['group', 'ns']).agg(pl.all().sort_by(metric).first())\n",
    "index_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for the best methods\n",
    "# Get rows that match the best label/ns combinations\n",
    "best_labels = index_best.select(['group', 'label', 'ns'])\n",
    "df_methods = df.join(best_labels, on=['group', 'label', 'ns'], how='inner')\n",
    "df_methods = df_methods.select(['group', 'label', 'ns', 'tau', 'cov', 'colors'] + cols + [metric])\n",
    "\n",
    "# Makes the curves monotonic. Cumulative max on the last column of the cols variable, e.g. \"pr\" --> precision\n",
    "if cumulate:\n",
    "    if metric in ['f', 'f_w', 'f_micro', 'f_micro_w']:\n",
    "        df_methods = df_methods.with_columns(\n",
    "            pl.col(cols[-1]).cum_max().over(['label', 'ns']).alias(cols[-1])\n",
    "        )\n",
    "    else:\n",
    "        df_methods = df_methods.with_columns(\n",
    "            pl.col(cols[-1]).cum_min().over(['label', 'ns']).alias(cols[-1])\n",
    "        )\n",
    "\n",
    "# Save to file\n",
    "df_methods.drop('colors').write_csv('{}/fig_{}.tsv'.format(out_folder, metric), separator=\"\\t\", float_precision=3)\n",
    "df_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add first last points to precision and recall curves to improve APS calculation\n",
    "def add_points(df_group):\n",
    "    # Get first and last rows\n",
    "    first_row = df_group.head(1).with_columns([\n",
    "        pl.lit(0.0).alias('tau'),\n",
    "        pl.lit(1.0).alias(cols[0]),\n",
    "        pl.lit(0.0).alias(cols[1]),\n",
    "    ])\n",
    "    last_row = df_group.tail(1).with_columns([\n",
    "        pl.lit(1.1).alias('tau'),\n",
    "        pl.lit(0.0).alias(cols[0]),\n",
    "        pl.lit(1.0).alias(cols[1]),\n",
    "    ])\n",
    "    return pl.concat([first_row, df_group, last_row])\n",
    "\n",
    "if metric.startswith('f') and add_extreme_points:\n",
    "    df_methods = df_methods.group_by(['group', 'label', 'ns'], maintain_order=True).map_groups(add_points)\n",
    "df_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for the best method and threshold\n",
    "df_best = index_best.select(['group', 'label', 'ns', 'tau', 'cov', 'colors'] + cols + [metric])\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average precision score \n",
    "if metric.startswith('f'):\n",
    "    # Calculate APS per group\n",
    "    aps_df = df_methods.sort(['group', 'label', 'ns', 'tau']).group_by(['group', 'label', 'ns']).agg(\n",
    "        ((pl.col(cols[0]).diff(-1).shift(1)) * pl.col(cols[1])).sum().alias('aps')\n",
    "    )\n",
    "    df_best = df_best.join(aps_df, on=['group', 'label', 'ns'], how='left')\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the max coverage across all thresholds\n",
    "max_cov_df = df_methods.group_by(['group', 'label', 'ns']).agg(\n",
    "    pl.col('cov').max().alias('max_cov')\n",
    ")\n",
    "df_best = df_best.join(max_cov_df, on=['group', 'label', 'ns'], how='left')\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a label column for the plot legend\n",
    "if 'aps' not in df_best.columns:\n",
    "    df_best = df_best.with_columns(\n",
    "        (pl.col('label') + ' (' + metric.upper() + '=' + pl.col(metric).round(3).cast(pl.Utf8) + ' C=' + pl.col('max_cov').round(3).cast(pl.Utf8) + ')').alias('plot_label')\n",
    "    )\n",
    "else:\n",
    "    df_best = df_best.with_columns(\n",
    "        (pl.col('label') + ' (' + metric.upper() + '=' + pl.col(metric).round(3).cast(pl.Utf8) + ' APS=' + pl.col('aps').round(3).cast(pl.Utf8) + ' C=' + pl.col('max_cov').round(3).cast(pl.Utf8) + ')').alias('plot_label')\n",
    "    )\n",
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the figures\n",
    "plt.rcParams.update({'font.size': 22, 'legend.fontsize': 18})\n",
    "\n",
    "# F-score contour lines\n",
    "x = np.arange(0.01, 1, 0.01)\n",
    "y = np.arange(0.01, 1, 0.01)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = 2 * X * Y / (X + Y)\n",
    "\n",
    "for ns in df_best['ns'].unique().to_list():\n",
    "    df_g = df_best.filter(pl.col('ns') == ns)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    # Contour lines. At the moment they are provided only for the F-score\n",
    "    if metric.startswith('f'):\n",
    "        CS = ax.contour(X, Y, Z, np.arange(0.1, 1.0, 0.1), colors='gray')\n",
    "        ax.clabel(CS, inline=True)\n",
    "\n",
    "    # Sort by metric and max_cov\n",
    "    if metric.startswith('f'):\n",
    "        df_g = df_g.sort([metric, 'max_cov'], descending=[True, True])\n",
    "    else:\n",
    "        df_g = df_g.sort([metric, 'max_cov'], descending=[False, True])\n",
    "\n",
    "    # Iterate methods\n",
    "    for i, row in enumerate(df_g.iter_rows(named=True)):\n",
    "        group = row['group']\n",
    "        label = row['label']\n",
    "        \n",
    "        data = df_methods.filter(\n",
    "            (pl.col('group') == group) & \n",
    "            (pl.col('label') == label) & \n",
    "            (pl.col('ns') == ns)\n",
    "        )\n",
    "        \n",
    "        # Precision-recall or mi-ru curves\n",
    "        ax.plot(data[cols[0]].to_numpy(), data[cols[1]].to_numpy(), \n",
    "                color=row['colors'], label=row['plot_label'], lw=2, zorder=500-i)\n",
    "        \n",
    "        # F-max or S-min dots\n",
    "        ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=12, mfc='none', zorder=1000-i)\n",
    "        ax.plot(row[cols[0]], row[cols[1]], color=row['colors'], marker='o', markersize=6, zorder=1000-i)\n",
    "\n",
    "    # Set axes limit\n",
    "    if metric.startswith('f'):\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "    # Set titles\n",
    "    ax.set_title(ontology_dict.get(ns, ns), pad=20)\n",
    "    ax.set_xlabel(axis_title_dict[cols[0]], labelpad=20)\n",
    "    ax.set_ylabel(axis_title_dict[cols[1]], labelpad=20)\n",
    "    \n",
    "    # Legend\n",
    "    leg = ax.legend(markerscale=6)\n",
    "    for legobj in leg.get_lines():\n",
    "        legobj.set_linewidth(10.0)\n",
    "\n",
    "    # Save figure on disk\n",
    "    plt.savefig(\"{}/fig_{}_{}.png\".format(out_folder, metric, ns), bbox_inches='tight', dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
